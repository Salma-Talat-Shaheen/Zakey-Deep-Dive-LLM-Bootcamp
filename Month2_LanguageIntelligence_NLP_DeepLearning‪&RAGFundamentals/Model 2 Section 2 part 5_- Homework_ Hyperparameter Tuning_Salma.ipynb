{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1QWMNIqrordH71QD2ql21n_E-TRDdeybw","timestamp":1760977963506}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# üìù Week 6 Homework: Hyperparameter Tuning\n","\n","**Goal**: Step into the role of a machine learning engineer by experimenting with hyperparameters to see how they affect model performance.\n","\n","---"],"metadata":{"id":"_OuUwTRiW3PV"}},{"cell_type":"markdown","source":["\n","\n","## ‚ñ∂Ô∏è Today's Video\n","\n","If you haven't already, watch this video to understand hyperparameters and why tuning them is one of the most important and creative skills in machine learning.\n","\n","üîó [Neural Networks Summary: All hyperparameters](https://www.youtube.com/watch?v=h291CuASDno)\n","\n","---"],"metadata":{"id":"7svRooOjW8zD"}},{"cell_type":"code","source":["#@title üîó Neural Networks Summary: All hyperparameters\n","from IPython.display import HTML\n","\n","\n","# Create the HTML for embedding\n","html_code = f\"\"\"\n","\n","<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/h291CuASDno?si=iko_Nw8BeGjsY0v_\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>\n","\n","\"\"\"\n","# Display the video\n","display(HTML(html_code))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":336},"cellView":"form","id":"_eF8W9WeW9n6","executionInfo":{"status":"ok","timestamp":1759030226992,"user_tz":-480,"elapsed":69,"user":{"displayName":"Mahmoud Sammour","userId":"07606788054442128619"}},"outputId":"dbe43f28-35ae-4213-ff6c-a641f7637832"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","\n","<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/h291CuASDno?si=iko_Nw8BeGjsY0v_\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>\n","\n"]},"metadata":{}}]},{"cell_type":"markdown","source":["\n","\n","## üìñ Today's Theory: The Art of Tuning\n","\n","The values we set ourselves **before training begins**‚Äîlike the **learning rate (`lr`)**, the **number of epochs**, or the **choice of optimizer**‚Äîare called **hyperparameters**. They control the learning process itself.\n","\n","Finding a good combination of hyperparameters is often more of an **art than a science** and is a critical skill for building high-performing models.\n","\n","### üìª Analogy: Tuning a Radio\n","\n","Finding the right learning rate is like tuning an old radio:\n","- Turn the dial **too quickly** (`lr` too high) ‚Üí you overshoot the signal and get noise.\n","- Turn it **too slowly** (`lr` too low) ‚Üí it takes forever to find a clear station.\n","\n","The goal is to find the **sweet spot** where learning is stable and efficient.\n","\n","---\n","\n","## üöÄ Your Task: Experiment, Document, and Analyze\n","\n","Use the **baseline script** (provided below) as your starting point. It‚Äôs a complete, working training and evaluation pipeline for MNIST.\n","\n","### üî¨ The Experiments\n","\n","Run **three separate experiments**. For each:\n","\n","1. Create a **new code cell**.\n","2. Copy the **entire baseline script** into it.\n","3. Make **only the specified change**.\n","4. Run the cell and record the **final test accuracy**.\n","\n","> üí° **Important**: Only change the hyperparameter listed for each experiment. Keep everything else identical.\n","\n","- **Experiment A (High Learning Rate)**: Change `lr` from `0.01` to `0.1`.\n","- **Experiment B (More Epochs)**: Change `epochs` from `3` to `10`.\n","- **Experiment C (Different Optimizer)**: Replace  \n","  `optim.SGD(net.parameters(), lr=0.01, momentum=0.9)`  \n","  with  \n","  `optim.Adam(net.parameters(), lr=0.001)`.\n","\n","---\n","\n","## üìã Homework Submission Template\n","\n","At the **very bottom of your notebook**, create a **new Markdown cell** and use this template to document your findings.\n","\n","### My Hyperparameter Tuning Experiments\n","\n","**Experiment A: Learning Rate (0.1)**  \n","- **Final Accuracy**: *What was the accuracy on the test set?*  \n","- **Conclusion**: *How did this high learning rate affect the model's ability to generalize compared to the original model?*\n","\n","**Experiment B: Epochs (10)**  \n","- **Final Accuracy**: *What was the accuracy after 10 epochs?*  \n","- **Conclusion**: *Did training for longer improve performance significantly? What is the trade-off with training time?*\n","\n","**Experiment C: Optimizer (Adam)**  \n","- **Final Accuracy**: *What was Adam's final accuracy?*  \n","- **Conclusion**: *How did Adam's performance compare to SGD's? Which would you choose for this problem and why?*"],"metadata":{"id":"YqnCOuh-SyO5"}},{"cell_type":"code","source":["# ========== Week 6 Homework: Hyperparameter Tuning ==========\n","\n","# --- INSTRUCTIONS ---\n","# For each experiment (A, B, C):\n","# 1. Copy this ENTIRE cell into a NEW code cell.\n","# 2. Make ONLY the specified change (see homework instructions).\n","# 3. Run the cell and note the final accuracy.\n","\n","# 1. Imports\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","import torchvision.transforms as transforms\n","\n","# 2. Load Data\n","transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n","trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n","testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n","\n","# 3. Define Model\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.fc1 = nn.Linear(28 * 28, 128)\n","        self.relu = nn.ReLU()\n","        self.fc2 = nn.Linear(128, 10)\n","    def forward(self, x):\n","        x = x.view(-1, 28 * 28)\n","        x = self.fc1(x)\n","        x = self.relu(x)\n","        x = self.fc2(x)\n","        return x\n","\n","# 4. Define Evaluation Function\n","def evaluate_model(model, loader):\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for data in loader:\n","            images, labels = data\n","            outputs = model(images)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","    return 100 * correct / total\n","\n","# ========== SOLUTION: Baseline (lr=0.01, epochs=3, SGD) ==========\n","# Expected accuracy: ~85-90%\n","\n","# ... (same imports and data loading as above) ...\n","\n","net = Net()\n","optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n","epochs = 3\n","# ... rest unchanged ‚Üí accuracy ‚âà 88.5%\n","\n","# ========== SOLUTION: Experiment A (lr=0.1) ==========\n","optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9)\n","epochs = 3\n","# ‚Üí Likely unstable, accuracy drops (e.g., ~10-50%)\n","\n","# ========== SOLUTION: Experiment B (epochs=10) ==========\n","optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n","epochs = 10\n","# ‚Üí Accuracy improves slightly (e.g., ~90-92%), but diminishing returns\n","\n","# ========== SOLUTION: Experiment C (Adam, lr=0.001) ==========\n","optimizer = optim.Adam(net.parameters(), lr=0.001)\n","epochs = 3\n","# ‚Üí Faster convergence, higher accuracy (e.g., ~92-94%)\n","\n","criterion = nn.CrossEntropyLoss()\n","\n","print(\"üöÄ Starting Training...\")\n","for epoch in range(epochs):\n","    for data in trainloader:\n","        inputs, labels = data\n","        optimizer.zero_grad()\n","        outputs = net(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","print(\"üèÅ Finished Training!\")\n","\n","accuracy = evaluate_model(net, testloader)\n","print(f'Final Test Accuracy: {accuracy:.2f} %')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JYaaZzwgWYGj","executionInfo":{"status":"ok","timestamp":1759029989805,"user_tz":-480,"elapsed":58193,"user":{"displayName":"Mahmoud Sammour","userId":"07606788054442128619"}},"outputId":"dd7408b2-898e-4b32-bc2d-51a7937c0d5d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["üöÄ Starting Training...\n","üèÅ Finished Training!\n","Final Test Accuracy: 96.22 %\n"]}]},{"cell_type":"markdown","source":["---\n","### **`My Solution and Explanation`**\n","---"],"metadata":{"id":"xBIjvfw_nKWn"}},{"cell_type":"code","source":["# ========== Week 6 Homework: Hyperparameter Tuning ==========\n","\n","# 1. Imports\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","import torchvision.transforms as transforms\n","\n","# 2. Load Data\n","transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n","trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n","testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n","\n","# 3. Define Model\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.fc1 = nn.Linear(28 * 28, 128)\n","        self.relu = nn.ReLU()\n","        self.fc2 = nn.Linear(128, 10)\n","    def forward(self, x):\n","        x = x.view(-1, 28 * 28)\n","        x = self.fc1(x)\n","        x = self.relu(x)\n","        x = self.fc2(x)\n","        return x\n","\n","# 4. Define Evaluation Function\n","def evaluate_model(model, loader):\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for data in loader:\n","            images, labels = data\n","            outputs = model(images)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","    return 100 * correct / total\n","\n","# ============================================================\n","# ========== SOLUTION: Baseline (lr=0.01, epochs=3, SGD) ==========\n","# Expected accuracy: ~85-90%\n","# ============================================================\n","net = Net()\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n","epochs = 3  # Baseline epochs\n","\n","print(\"üöÄ Starting Training (Baseline)...\")\n","for epoch in range(epochs):\n","    for data in trainloader:\n","        inputs, labels = data\n","        optimizer.zero_grad()\n","        outputs = net(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","print(\"üèÅ Finished Training (Baseline)!\")\n","baseline_acc = evaluate_model(net, testloader)\n","print()\n","print(f'Baseline Final Test Accuracy: {baseline_acc:.2f} %')\n","print()\n","# ============================================================\n","# ========== SOLUTION: Experiment A (High LR) ==========\n","# lr = 0.1\n","# ============================================================\n","net_a = Net()\n","optimizer_a = optim.SGD(net_a.parameters(), lr=0.1, momentum=0.9)\n","epochs_a = 3\n","\n","for epoch in range(epochs_a):\n","    for data in trainloader:\n","        inputs, labels = data\n","        optimizer_a.zero_grad()\n","        outputs = net_a(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer_a.step()\n","acc_a = evaluate_model(net_a, testloader)\n","print(f'Experiment A Accuracy (High LR): {acc_a:.2f} %')\n","\n","# ============================================================\n","# ========== SOLUTION: Experiment B (More Epochs) ==========\n","# epochs = 10\n","# ============================================================\n","net_b = Net()\n","optimizer_b = optim.SGD(net_b.parameters(), lr=0.01, momentum=0.9)\n","epochs_b = 10\n","\n","for epoch in range(epochs_b):\n","    for data in trainloader:\n","        inputs, labels = data\n","        optimizer_b.zero_grad()\n","        outputs = net_b(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer_b.step()\n","acc_b = evaluate_model(net_b, testloader)\n","print(f'Experiment B Accuracy (More Epochs): {acc_b:.2f} %')\n","\n","# ============================================================\n","# ========== SOLUTION: Experiment C (Adam Optimizer) ==========\n","# optimizer = Adam, lr = 0.001\n","# ============================================================\n","net_c = Net()\n","optimizer_c = optim.Adam(net_c.parameters(), lr=0.001)\n","epochs_c = 3\n","\n","for epoch in range(epochs_c):\n","    for data in trainloader:\n","        inputs, labels = data\n","        optimizer_c.zero_grad()\n","        outputs = net_c(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer_c.step()\n","acc_c = evaluate_model(net_c, testloader)\n","print(f'Experiment C Accuracy (Adam): {acc_c:.2f} %')\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wKXO2C0CmbGn","executionInfo":{"status":"ok","timestamp":1760983834923,"user_tz":-180,"elapsed":302312,"user":{"displayName":"salma shaheen","userId":"13963103147545074551"}},"outputId":"df4807e0-e4b5-48de-cf75-7b21e44c30fd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["üöÄ Starting Training (Baseline)...\n","üèÅ Finished Training (Baseline)!\n","\n","Baseline Final Test Accuracy: 96.34 %\n","\n","Experiment A Accuracy (High LR): 77.69 %\n","Experiment B Accuracy (More Epochs): 97.46 %\n","Experiment C Accuracy (Adam): 96.62 %\n"]}]},{"cell_type":"markdown","source":["###  **My Hyperparameter Tuning Experiments**\n","\n","| **Experiment** | **Hyperparameter Change** | **Final Accuracy** | **Conclusion** |\n","|----------------|----------------------------|--------------------|----------------|\n","| **A** | Learning Rate = 0.1 | 77.69% | High learning rate caused unstable training. The model overshot optimal weights, reducing accuracy. |\n","| **B** | Epochs = 10 | 97.46% | Training longer improved performance significantly. Diminishing returns are small, but training time increases. |\n","| **C** | Optimizer = Adam (lr=0.001) | 96.62% | Adam converged faster and achieved higher accuracy compared to SGD, making it efficient for this dataset. |\n","\n","---\n","\n","### **Reflection**\n","Through these experiments, I learned that **small hyperparameter changes can greatly affect model performance**.  \n","- A **too-high learning rate** made training unstable.  \n","- **More epochs** allowed the model to learn deeper patterns and reach higher accuracy.  \n","- The **Adam optimizer** provided faster convergence and better performance compared to SGD.  \n","\n","This exercise helped me understand how tuning hyperparameters is both a **science and an art** ‚Äî finding the perfect balance can make a simple model perform impressively well.\n"],"metadata":{"id":"SbhVKJwCvacC"}},{"cell_type":"code","source":["#@title Run to Enter your results\n","\n","# ========== Record Your Homework Results ==========\n","# Run this cell to input your experiment accuracies interactively\n","\n","try:\n","    expA = float(input(\"Enter final test accuracy for Experiment A (High LR = 0.1): \"))\n","    expB = float(input(\"Enter final test accuracy for Experiment B (Epochs = 10): \"))\n","    expC = float(input(\"Enter final test accuracy for Experiment C (Adam optimizer): \"))\n","\n","    # Validate ranges\n","    if not all(0 <= acc <= 100 for acc in [expA, expB, expC]):\n","        print(\"‚ö†Ô∏è Warning: Accuracy should be between 0 and 100. Please re-run this cell if values are incorrect.\")\n","\n","    # Store in the expected format for self-assessment\n","    homework_results = {\n","        'expA_acc': expA,\n","        'expB_acc': expB,\n","        'expC_acc': expC\n","    }\n","\n","    print(\"\\n‚úÖ Results saved successfully!\")\n","    print(f\"Experiment A: {expA:.2f}%\")\n","    print(f\"Experiment B: {expB:.2f}%\")\n","    print(f\"Experiment C: {expC:.2f}%\")\n","\n","except ValueError:\n","    print(\"‚ùå Error: Please enter numeric values only (e.g., 85.3). Re-run this cell to try again.\")\n","    homework_results = None"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"taNL-lKBYSaW","executionInfo":{"status":"ok","timestamp":1760983892377,"user_tz":-180,"elapsed":22065,"user":{"displayName":"salma shaheen","userId":"13963103147545074551"}},"outputId":"36a5ff85-7f22-4fa0-c33c-2041c7254843"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Enter final test accuracy for Experiment A (High LR = 0.1): 77.69\n","Enter final test accuracy for Experiment B (Epochs = 10): 97.46\n","Enter final test accuracy for Experiment C (Adam optimizer): 96.62\n","\n","‚úÖ Results saved successfully!\n","Experiment A: 77.69%\n","Experiment B: 97.46%\n","Experiment C: 96.62%\n"]}]},{"cell_type":"code","source":["# ========== Week 6 Homework Self-Assessment ==========\n","\n","#@title Run to check your homework submission\n","from IPython.display import display, Markdown\n","import re\n","\n","def check_homework_submission():\n","    feedback = []\n","    score = 0\n","    total = 1\n","\n","    # Check if a markdown cell with results exists below\n","    # (We can't programmatically read other markdown cells in Colab/Jupyter,\n","    # so we ask the student to define a variable with their results.)\n","\n","    # ALTERNATIVE: Ask student to define a dict in a code cell after experiments\n","    try:\n","        # Student should create this after running all 3 experiments\n","        if 'homework_results' in globals():\n","            results = homework_results\n","            required_keys = {'expA_acc', 'expB_acc', 'expC_acc'}\n","            if not required_keys.issubset(results.keys()):\n","                feedback.append(\"‚ùå Please define `homework_results` with keys: 'expA_acc', 'expB_acc', 'expC_acc'\")\n","            else:\n","                # Basic sanity check: accuracies should be between 0 and 100\n","                valid = all(0 <= v <= 100 for v in [results['expA_acc'], results['expB_acc'], results['expC_acc']])\n","                if valid:\n","                    score += 1\n","                    feedback.append(\"‚úÖ Homework results recorded correctly!\")\n","                else:\n","                    feedback.append(\"‚ùå Accuracy values must be between 0 and 100.\")\n","        else:\n","            feedback.append(\"üìù **Reminder**: After running all 3 experiments, create a code cell with:\\n```python\\nhomework_results = {\\n    'expA_acc': YOUR_ACCURACY_A,\\n    'expB_acc': YOUR_ACCURACY_B,\\n    'expC_acc': YOUR_ACCURACY_C\\n}\\n```\")\n","    except Exception as e:\n","        feedback.append(f\"‚ùå Error checking results: {e}\")\n","\n","    final_message = \"**üéØ Week 6 Homework Self-Assessment**\\n\\n\" + \"\\n\".join(feedback)\n","    final_message += f\"\\n\\nüìä **Score: {score}/{total}**\"\n","    if score == 1:\n","        final_message += \"\\n\\nüéâ Great! You‚Äôve completed the hyperparameter tuning homework. Well done!\"\n","    else:\n","        final_message += \"\\n\\n‚úèÔ∏è Please follow the instructions above to record your results.\"\n","\n","    display(Markdown(final_message))\n","\n","check_homework_submission()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":116},"cellView":"form","id":"Y2knxkMKWunt","executionInfo":{"status":"ok","timestamp":1760983895489,"user_tz":-180,"elapsed":64,"user":{"displayName":"salma shaheen","userId":"13963103147545074551"}},"outputId":"dd301407-19dd-4eea-fbb6-2d9754c87676"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**üéØ Week 6 Homework Self-Assessment**\n\n‚úÖ Homework results recorded correctly!\n\nüìä **Score: 1/1**\n\nüéâ Great! You‚Äôve completed the hyperparameter tuning homework. Well done!"},"metadata":{}}]}]}