{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üß† Model 3 Section 1: Homework\n",
        "> ## Building a Full RAG Pipeline\n",
        "\n",
        "> **üéØ Today‚Äôs Goal**: Combine all the parts from Section 1. We will use the **Retriever** (MiniLM) and the **Generator** (DistilBERT) to build a complete, end-to-end Retrieval-Augmented Generation (RAG) system.\n",
        "\n",
        "---\n",
        "\n",
        "###  recap: The Two Parts of Our RAG System\n",
        "\n",
        "1.  **The Retriever (Part 2)** üîé\n",
        "    * **Model:** `all-MiniLM-L6-v2`\n",
        "    * **Job:** To turn a text query into a vector and use **semantic search** to find the *most relevant* piece of context from our knowledge base.\n",
        "\n",
        "2.  **The Generator (Part 3)** ‚úçÔ∏è\n",
        "    * **Model:** `distilbert-base-cased-distilled-squad`\n",
        "    * **Job:** To take a `question` and a `context` and **extract** the specific answer from within the context.\n",
        "\n",
        "Today, we connect them. The output of the Retriever becomes the input for the Generator.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Bh20Fd7ypN9A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "### üß† Step 1: Load Models and Knowledge\n",
        "\n",
        "Now, let's load both of our specialized models and define our knowledge base. We have one model for retrieving and one for generating.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "vz8_wQuapVMj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code Cell 2: Load Models (The \"CPU/GPU Split\" Fix)\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "# --- 1. Load the Retriever (MiniLM) on the CPU ---\n",
        "# We explicitly tell it to use the 'cpu'.\n",
        "# This is fast enough for a retriever and saves all our VRAM.\n",
        "retriever_model = SentenceTransformer(\n",
        "    'all-MiniLM-L6-v2',\n",
        "    device='cpu'  # Force to CPU\n",
        ")\n",
        "print(f\"‚úÖ Retriever model (MiniLM) loaded. Using device: cpu\")\n",
        "\n",
        "\n",
        "# --- 2. Load the Generator (DistilBERT) on the GPU ---\n",
        "# We check if a GPU is available and set the device index\n",
        "# 0 = first GPU, -1 = CPU\n",
        "pipeline_device = 0 if torch.cuda.is_available() else -1\n",
        "\n",
        "generator_model = pipeline(\"question-answering\",\n",
        "                           model=\"distilbert-base-cased-distilled-squad\",\n",
        "                           device=pipeline_device) # Use GPU if available\n",
        "\n",
        "print(f\"‚úÖ Generator model (DistilBERT) loaded.\")\n",
        "if pipeline_device == 0:\n",
        "    print(\"   -> Running on GPU (Good!)\")\n",
        "else:\n",
        "    print(\"   -> WARNING: Running on CPU (Will be slow, but should work)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 723,
          "referenced_widgets": [
            "293256b7f48c4437807840c2b9594d4c",
            "6c59a100b5124931b059f3709f6f8e35",
            "414dcf1b3259473ebfb3e3b029ece97a",
            "e70352f9764446d9b68eff331e27fab8",
            "c688547feb97440889b2016c16adbaf8",
            "080aae919e434c04bb6ed771d25873b8",
            "22312470353842d68f5efb835dadd9c7",
            "fffe291f1abe42449c02fb8f552ecc41",
            "bc759ab799e449619c2790e2e80980b3",
            "50b0d3dfbf274f399cd376687004b206",
            "e8c627ac1bd64ebbab2a81a068dacf7c",
            "5b52702c017545bc86b2a8653ced04b6",
            "ebb041a2c7ea4045a9857cd9370fb8c3",
            "09484c84914949eb917011630a1e43ec",
            "0ec15d8274654b9aa2338e96f36f064b",
            "627be0c9eaba48a5a9e01e41c5a0edad",
            "eba902705a6249deb6ae623f631a1d03",
            "043ced9e99904b9d82079bd951ddad43",
            "c0970f1e024f456d8573cb4147314478",
            "ccfc05c9998547829bd7cd00eeb909ac",
            "4bb94540a66c4ed49acfe3a8a5352fb4",
            "0350aa5fd2e6442bafd231e3a120245c",
            "ef3cf0dd04ea4f359ca53ff33bc0c34b",
            "9cbb640643c24f17afb451abf7736b3b",
            "a88248717b754475ae37d74e84a6ab6e",
            "4c9cdc5cee6241e9b92b433fdafa8eb3",
            "01a4c3bc3cf44fa1995f375f4adbdf77",
            "3357dd658c5d45f98127156bc737faac",
            "b43e9174bb764259bdb92d8936a81275",
            "849f4b253f5d4f6a9f5979ea52a36753",
            "2fac3aa539c04172874a832f9150165b",
            "8ad7dd633f9c450da00a31dd845a126e",
            "be0543a0befc4c9aa49e05971db4be58",
            "f34e58b3e4264fd3a761792244fa18bd",
            "fa899784d7c0450a9049f540658a0a68",
            "23d97d81eccb49e882caa4f81d98830d",
            "4a034726e28b40f58c371cc12e816c5e",
            "055c8ed455ce4ef8ae49425e9fa222bb",
            "95413a955be04c32892069d97a0e1e14",
            "3a6aac9772bd4fb8aca263f7b5bcdf5a",
            "beb99d053033486ba14942b0c8c31f2a",
            "14bf5a3e982d4388932a5da04f8a3795",
            "3e915649cf804c4b907ab00fe0227956",
            "bff32cec35994071ac1ebc68fa4cccfb",
            "307ad26d6527403b8c38217770d3b932",
            "dd77791720bf4978906ac5d18ba0dad0",
            "0b63e26acdcf4021aa0afa462dd0bbe7",
            "24fc6dfb4e844130a7c8a82d0c7f992d",
            "235080cfaa4842fc99eb4563e211a88a",
            "aff69d53e85d4d69ae5568d7a582c86f",
            "e52862b15ad44ed7af295a47eccb6006",
            "fd7b375035cb4712b11adeb5f4e82a68",
            "9ae1a310bbf144b5a837963a9260045c",
            "7c61db86633e41eea7230897a7ca65f4",
            "536eb62be2424c90b802582f4ec7dd52",
            "ef0fc6b391424e8180ecaf5689499f18",
            "2763eb72f0b54839a740131745cdf3ae",
            "b4885dd0c8654b84933c61c139ae7533",
            "935315847a7d4d82b3b1eba9503836c6",
            "0d3e97d01b364f2d9a190964e36d1239",
            "83abdd7c360e4af4b7ba056f7d20febe",
            "40be345fd8504a7d8e0ffbc2d1214b58",
            "eec083a74c2347e3a463b05fde529ca1",
            "73117e93ee2b4f8d89b9c34650aeb724",
            "b1f1b3e733c345298897be9ff628ce64",
            "7da67214805e495fa9fb9063a71c11d5",
            "0cc310036dc84821bb68463fec301d10",
            "9b610da06dfd4613a17fd4bb814bd3d0",
            "4b8063dcec074a9195c4e3d58181980a",
            "bed41497785a492a8fae874b0b9ea579",
            "53b6ca48e0404f57b329a702c2855dc7",
            "603e804eec8944f3af51bfc320839c3e",
            "139e3c17fd654e0f8cdf00ad21798aaf",
            "830da6cea4574fa3904e4de29569c3d4",
            "f268c2562cb0415da37c293c5dbe68b5",
            "ce5eb8bdefa940f0b9cb04ec10d8c97f",
            "23e9cbeef5a74b9a9b1ac0c14ddb779e",
            "102d64d181e4420897edc7cbf62e5baa",
            "c8792d8ddde24a36ab9ea001305a3f05",
            "eae975e513ae4054b65305d5634a5004",
            "fcb893428232421b8d254e8779378844",
            "b13288f197034c91b09eadefc4dd25ed",
            "68735ce4430449f491d16d0f67a3a602",
            "added83d55624f50a82399525feeca63",
            "f35f07fd6a3a4b7facaa34083f82bee9",
            "55bc8924d0c3457c9b343cdbbcb9ab97",
            "e75fa93b18fd4b36acad4c032b2b8958",
            "f48aebb7b6a64d5da63f7f6f789d1e75",
            "c6bb3c5e58514a7cae84e1857ef62863",
            "3e111824b8e74facaeab5d6e6c671fa5",
            "b4dd09b1eb6e47e38b4ed2b7e7cc013e",
            "a922d942373f4fcab57e23a9a17d2d58",
            "0255592355ad4b5da31163e4481bb73a",
            "f0d0caff972641fbb786b052f61cf09e",
            "bb9895b57afc4a5889ef5588a2bac791",
            "6867bfca64a2476eb1804985f4894e77",
            "776d01ae7ce646b59891275fe910c5cf",
            "a31bde47bccf4aeaa14962dfe3f9aa01",
            "5256591856c346db94aa7fb9941c0d39",
            "1ddca8d3fcfc4fe1b7afbc4b3bd718ec",
            "a0f62fb26b9c489797d7a328035861bc",
            "f434102af0e740288dae912a3d6f293d",
            "9d204b6d57ca42aabf9184217981d751",
            "2cca0430b91841809df60985c4918cce",
            "da95f5ce88f344bd8f9db6c51d004bdb",
            "8d17696c05fd49e4a9507643942a7f9c",
            "2dcf6b0c79914f50a27c08404ddde524",
            "045dc1fcfb89479ab175677f6866dcfb",
            "2c9fe3f9fd4e4e1db8036613b5b1bcb3",
            "1d53f6d0d4d445e8b7cb86c64754fc57",
            "b1b1d34c2f0349c8a49ef72097c2c6d8",
            "9123c9cf60554537a7471dfe0d28965b",
            "642bd2f645a54a429980788bffe58cc6",
            "d0b2f8aab8cb44ffaa3f79b7ef50c32d",
            "79fec1c2bed2413899376c54bea55f73",
            "ebc629715c9b4beb944371908fe8c3a7",
            "20ee1a6a2a094f30a267918f3e6246f8",
            "45101108b7924f1aa355a7b60246562b",
            "8b492a6a93c04979a5dcdc9bfad948f7",
            "0f15d5d0de1a4cc3a464d768d56c9403",
            "2dba21a73a6c4fb494f087409fbe76fb",
            "9b7aa07a787c4ec798b456d893f9d8b7",
            "6fc43686c05a4168bb4b68e7cdc69421",
            "dbcac47ecc834c18ac6c4531bb2cff15",
            "3b609b54776a48bdab2897f294ef031e",
            "57acbd60439543ffac9097110c653824",
            "6cc3f5c6c44c4c919f137dccce329923",
            "4b8bf156a59f42a5879de61ab8de9ddc",
            "25eea7b3fe494141bedcb8d0311149e6",
            "0df914bc7bcc4e9facf9a4bced636559",
            "a069711436db4783a294c544e067db8f",
            "463ed7c4989d458db0c8d5a1ba7dbf91",
            "25ae593ee8184dc386a9deddafd6a035",
            "ec9e0411f6aa4cd1b0797f08a121e243",
            "adea67761a914fd8aca15180c7bfcce8",
            "0cdbb813cf204e19948298466a95c9cb",
            "906b5b1b8f454551a374ef939ff245bd",
            "fdc50cee16fa4ccaa6ac31ab68ddfb0b",
            "eb3f936839b74d08ac050da3fc157205",
            "e87509884ee7445ca7b0e81934150bd2",
            "f6e58bce2f7d4148b9b29a19be255811",
            "522c224ab03b4cd38c828cd16b854648",
            "39d99e38fe9f4a1595c864bc7ad5993a",
            "6adec1afd33d41968fed61fb044a15ac",
            "aef13848485f44e7a26870ac40ea884e",
            "0d1af43b88954beb9ed8419b3ab83e96",
            "bf2008d62cb9449fa75f3f3e0b7e6755",
            "cd4b90387c2c44ee958eff6ed118e427",
            "364c064bd07a4ff398691eb3af6e5109",
            "1280e37cf3dc4123b24477c299c13a20",
            "393ce2ec677145d7a7970bee04851352",
            "4e68fa8cefbf4d80b8e8ac4c49d5dcb9",
            "8a0875579ad44bd0a7f97b6c31cc9874",
            "eccc2b37c987442ca85bb9a4f2f658d3",
            "edd490d88b8945f7893ab19fd8706abc",
            "caf2bb6849314f20bf5ca5c4327b947f",
            "7834d80ab2114e609bc9a022065949fd",
            "4767e560100c4156a1af26bb33894271",
            "e5559bf9bcee4a0698978882d0bafcaa",
            "cc3630970e1a462386100b066aa556f1",
            "ed8ce408fe224baeaf7873903226d569",
            "9cf910fdfbfe402ba77f9f12d8bf2ffd",
            "8e2e8bbfdf194e339b7b3b03212530b8",
            "01ae58b1cc874e8ea89d0330fa483da9",
            "a299b48f57214bf0aece64c2cfcaf2dd",
            "37d1c26ce7c149fdba4483e1ac7d59be",
            "eb103496813540f285bfed23a4cfd900",
            "dfdabea4646045ce8366e437bb85d39b",
            "dfd65bb5e9b14950b39efedbae0be780",
            "2067ed75677941f4844897b8f4305cb2",
            "80ea41f05e8c4a8899abff00ddf24136",
            "611b8e55ba554f468d5ae15d7500d60e",
            "4215cdcf84f14434b968b36b90a17119",
            "28201ee3ff8e45eba12731c88166c201",
            "79f038a8055a474496d322c515e846fc",
            "5910d29601b2485b875fcfae48f14b0b"
          ]
        },
        "id": "vp1iXwIQpZn4",
        "outputId": "452f0db8-99ea-4ad1-b98c-d8b715b0eb52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "293256b7f48c4437807840c2b9594d4c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5b52702c017545bc86b2a8653ced04b6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ef3cf0dd04ea4f359ca53ff33bc0c34b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f34e58b3e4264fd3a761792244fa18bd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "307ad26d6527403b8c38217770d3b932"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ef0fc6b391424e8180ecaf5689499f18"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0cc310036dc84821bb68463fec301d10"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "102d64d181e4420897edc7cbf62e5baa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c6bb3c5e58514a7cae84e1857ef62863"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1ddca8d3fcfc4fe1b7afbc4b3bd718ec"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b1b1d34c2f0349c8a49ef72097c2c6d8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Retriever model (MiniLM) loaded. Using device: cpu\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9b7aa07a787c4ec798b456d893f9d8b7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "25ae593ee8184dc386a9deddafd6a035"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6adec1afd33d41968fed61fb044a15ac"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "edd490d88b8945f7893ab19fd8706abc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "37d1c26ce7c149fdba4483e1ac7d59be"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Generator model (DistilBERT) loaded.\n",
            "   -> Running on GPU (Good!)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### üìö Step 2: Define and Encode Knowledge Base\n",
        "\n",
        "Here is our simple knowledge base. We will give this \"long-term memory\" to our AI agent.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "VYrcM9FrpcAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code Cell 3: Define Knowledge Base\n",
        "# This is the \"long-term memory\" of our agent\n",
        "knowledge_base = [\n",
        "    \"Buddy is a 3-year-old Golden Retriever who loves to play fetch.\",\n",
        "    \"The capital of France is Paris, which is known for the Eiffel Tower.\",\n",
        "    \"Python is an interpreted, high-level, general-purpose programming language.\",\n",
        "    \"The first person to walk on the Moon was Neil Armstrong in 1969.\",\n",
        "    \"Climate change is the long-term alteration of temperature and typical weather patterns.\"\n",
        "]\n",
        "\n",
        "print(f\"üìö Knowledge base created with {len(knowledge_base)} documents.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOB-pcM3pedx",
        "outputId": "593cdee7-5b90-4906-cf4b-0e8fc357b635"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìö Knowledge base created with 5 documents.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### ‚úèÔ∏è Task 1: Encode Your Knowledge\n",
        "\n",
        "Your first task is to use the **Retriever model** (`retriever_model`) to encode all the documents in your `knowledge_base`.\n",
        "\n",
        "**Your Goal:** Create a variable called `knowledge_embeddings` that holds the vector representations of all your documents.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "rW8H65Pfph0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code Cell 4: Task 1 - Encode Knowledge\n",
        "print(\"--- Task 1: Encoding Knowledge Base ---\")\n",
        "\n",
        "# TODO: Use the retriever_model to encode the 'knowledge_base' list\n",
        "# The .encode() method takes a list of strings and returns a list of embeddings\n",
        "# Set convert_to_tensor=True\n",
        "knowledge_embeddings = retriever_model.encode(knowledge_base, convert_to_tensor=True) # <--- ÿ™ŸÖ ÿ™ŸÅÿπŸäŸÑ Ÿáÿ∞ÿß ÿßŸÑÿ≥ÿ∑ÿ±\n",
        "\n",
        "# --- Verification ---\n",
        "if 'knowledge_embeddings' in locals() and knowledge_embeddings.shape[0] == len(knowledge_base):\n",
        "    print(\"‚úÖ Success! Knowledge base has been encoded.\")\n",
        "    print(f\" ¬† -> Embedding shape: {knowledge_embeddings.shape}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Task 1 not complete. 'knowledge_embeddings' not found or has wrong shape.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsyFiov3pinq",
        "outputId": "056a80cb-8e42-4758-e267-d8bc235c6beb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Task 1: Encoding Knowledge Base ---\n",
            "‚úÖ Success! Knowledge base has been encoded.\n",
            " ¬† -> Embedding shape: torch.Size([5, 384])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### ‚úèÔ∏è Task 2: Build the Retriever Function\n",
        "\n",
        "Now, let's build a function that performs the \"R\" (Retrieval) step. This function will take a user's `query` and find the most relevant document from our `knowledge_base`.\n",
        "\n",
        "**Your Goal:** Complete the `retrieve_context` function.\n",
        "1.  Encode the incoming `query` using the `retriever_model`.\n",
        "2.  Use `util.pytorch_cos_sim` to compare the `query_embedding` to all `knowledge_embeddings`.\n",
        "3.  Find the index of the highest-scoring document (use `torch.argmax`).\n",
        "4.  Return the *text* of that document from the `knowledge_base`.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "nJLvCx32pnX1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code Cell 5: Task 2 - Build the Retriever\n",
        "print(\"--- Task 2: Building the Retriever ---\")\n",
        "\n",
        "def retrieve_context(query):\n",
        "    # 1. Encode the query\n",
        "    # TODO: Encode the 'query' using the 'retriever_model'.\n",
        "    # Don't forget convert_to_tensor=True\n",
        "    # ÿßŸÑÿÆÿ∑Ÿàÿ© 1: ÿ™ÿ≠ŸàŸäŸÑ ÿßŸÑÿßÿ≥ÿ™ÿπŸÑÿßŸÖ ÿßŸÑŸÜÿµŸä ÿ•ŸÑŸâ ŸÖÿ™ÿ¨Ÿá\n",
        "    query_embedding = retriever_model.encode(query, convert_to_tensor=True)\n",
        "\n",
        "    # 2. Compute cosine similarity\n",
        "    # TODO: Use 'util.pytorch_cos_sim' to compare the 'query_embedding'\n",
        "    # with all 'knowledge_embeddings'\n",
        "    # ÿßŸÑÿÆÿ∑Ÿàÿ© 2: ÿ≠ÿ≥ÿßÿ® ÿßŸÑÿ™ÿ¥ÿßÿ®Ÿá ÿ®ŸäŸÜ ŸÖÿ™ÿ¨Ÿá ÿßŸÑÿßÿ≥ÿ™ÿπŸÑÿßŸÖ Ÿàÿ¨ŸÖŸäÿπ ŸÖÿ™ÿ¨Ÿáÿßÿ™ ŸÇÿßÿπÿØÿ© ÿßŸÑŸÖÿπÿ±ŸÅÿ©\n",
        "    # [0] ŸäŸèÿ≥ÿ™ÿÆÿØŸÖ ŸÑÿ≥ÿ≠ÿ® ÿµŸÅ ÿßŸÑŸÜÿ™ÿßÿ¶ÿ¨ ŸÖŸÜ ŸÖÿµŸÅŸàŸÅÿ© ÿßŸÑÿ™ÿ¥ÿßÿ®Ÿá ÿ´ŸÜÿßÿ¶Ÿäÿ© ÿßŸÑÿ£ÿ®ÿπÿßÿØ\n",
        "    cos_scores = util.pytorch_cos_sim(query_embedding, knowledge_embeddings)[0]\n",
        "\n",
        "    # 3. Find the best match\n",
        "    # TODO: Use 'torch.argmax' to find the index of the highest score\n",
        "    # ÿßŸÑÿÆÿ∑Ÿàÿ© 3: ÿ•Ÿäÿ¨ÿßÿØ ŸÅŸáÿ±ÿ≥ (Index) ÿßŸÑŸàÿ´ŸäŸÇÿ© ÿßŸÑÿ™Ÿä ÿ≠ÿµŸÑÿ™ ÿπŸÑŸâ ÿ£ÿπŸÑŸâ ÿØÿ±ÿ¨ÿ© ÿ™ÿ¥ÿßÿ®Ÿá\n",
        "    top_result_index = torch.argmax(cos_scores)\n",
        "\n",
        "    # 4. Return the matching document text\n",
        "    # TODO: Return the text from 'knowledge_base' at 'top_result_index'\n",
        "    # ÿßŸÑÿÆÿ∑Ÿàÿ© 4: ÿ•ÿ±ÿ¨ÿßÿπ ÿßŸÑŸÜÿµ ÿßŸÑŸÅÿπŸÑŸä ŸÑŸÑŸàÿ´ŸäŸÇÿ© ÿßŸÑÿ£ŸÅÿ∂ŸÑ ŸÖÿ∑ÿßÿ®ŸÇÿ©\n",
        "    return knowledge_base[top_result_index]\n",
        "\n",
        "# --- Verification ---\n",
        "print(\"Testing retrieve_context('What is Python?')...\")\n",
        "retrieved = retrieve_context(\"What is Python?\")\n",
        "print(f\" ¬† -> Retrieved: '{retrieved}'\")\n",
        "if \"Python\" in retrieved:\n",
        "    print(\"‚úÖ Success! Retriever function works.\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Retriever function failed to find the right document.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZF5eaEQpoPU",
        "outputId": "c2891d50-af69-42e4-db3a-794208b9c8de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Task 2: Building the Retriever ---\n",
            "Testing retrieve_context('What is Python?')...\n",
            " ¬† -> Retrieved: 'Python is an interpreted, high-level, general-purpose programming language.'\n",
            "‚úÖ Success! Retriever function works.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### ‚úèÔ∏è Task 3: Build the Generator Function\n",
        "\n",
        "Great! We have a function to get context. Now let's build a function for the \"G\" (Generation) step. This function will take a `question` and the `context` we just retrieved.\n",
        "\n",
        "**Your Goal:** Complete the `generate_answer` function.\n",
        "1.  Call the `generator_model` (which is a `pipeline` object).\n",
        "2.  Pass the `question` and `context` to it.\n",
        "3.  Return *only the answer* from the resulting dictionary (e.g., `result['answer']`).\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "ejGGnWtQprE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code Cell 6: Task 3 - Build the Generator\n",
        "print(\"\\n--- Task 3: Building the Generator ---\")\n",
        "\n",
        "def generate_answer(question, context):\n",
        "    # 1. Call the pipeline\n",
        "    # TODO: Call the 'generator_model' pipeline, passing in the\n",
        "    # 'question' and 'context'\n",
        "    # ÿßŸÑÿÆÿ∑Ÿàÿ© 1: ÿßÿ≥ÿ™ÿØÿπÿßÿ° ÿÆÿ∑ ÿ£ŸÜÿßÿ®Ÿäÿ® ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿπŸÑŸâ ÿßŸÑÿ£ÿ≥ÿ¶ŸÑÿ© (Question-Answering)\n",
        "    result = generator_model(question=question, context=context)\n",
        "\n",
        "    # 2. Return the answer\n",
        "    # TODO: Return the 'answer' part of the 'result' dictionary\n",
        "    # ÿßŸÑÿÆÿ∑Ÿàÿ© 2: ÿ•ÿ±ÿ¨ÿßÿπ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿßŸÑŸÖÿ≥ÿ™ÿÆÿ±ÿ¨ÿ© ŸÖŸÜ ÿßŸÑŸÇÿßŸÖŸàÿ≥ ÿßŸÑŸÜÿßÿ™ÿ¨\n",
        "    return result['answer']\n",
        "\n",
        "# --- Verification ---\n",
        "print(\"Testing generate_answer('What is Python?', '...')...\")\n",
        "test_context = \"Python is a popular programming language.\"\n",
        "test_question = \"What is Python?\"\n",
        "answer = generate_answer(test_question, test_context)\n",
        "print(f\" ¬† -> Question: '{test_question}'\")\n",
        "print(f\" ¬† -> Context: '{test_context}'\")\n",
        "print(f\" ¬† -> Answer: '{answer}'\")\n",
        "\n",
        "if \"popular programming language\" in answer:\n",
        "    print(\"‚úÖ Success! Generator function works.\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Generator function failed to extract the answer.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWeHgxZcpuN2",
        "outputId": "572213a8-5217-4b35-a1c2-c2d5b0925dbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Task 3: Building the Generator ---\n",
            "Testing generate_answer('What is Python?', '...')...\n",
            " ¬† -> Question: 'What is Python?'\n",
            " ¬† -> Context: 'Python is a popular programming language.'\n",
            " ¬† -> Answer: 'a popular programming language'\n",
            "‚úÖ Success! Generator function works.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "### üöÄ Task 4: Build the Full RAG Pipeline!\n",
        "\n",
        "This is the final step. Let's combine our two functions into a single, end-to-end RAG pipeline. This function will orchestrate the entire process.\n",
        "\n",
        "**Your Goal:** Complete the `ask_rag_pipeline` function.\n",
        "1.  Call your `retrieve_context` function to get the `best_context` for the `query`.\n",
        "2.  Call your `generate_answer` function, passing in the *original* `query` and the `best_context` you just found.\n",
        "3.  Return the final `answer`.\n",
        "\n",
        "After you write the function, we'll test it with a query!\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "WXuKM7bHpyM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code Cell 7: Task 4 - Build the Full RAG Pipeline\n",
        "print(\"\\n--- Task 4: Building the Full RAG Pipeline ---\")\n",
        "\n",
        "def ask_rag_pipeline(query):\n",
        "    # 1. Retrieve\n",
        "    # TODO: Call your 'retrieve_context' function\n",
        "    # ÿßŸÑÿÆÿ∑Ÿàÿ© 1: ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ ÿØÿßŸÑÿ© ÿßŸÑÿßÿ≥ÿ™ÿ±ÿ¨ÿßÿπ ŸÑŸÑÿ≠ÿµŸàŸÑ ÿπŸÑŸâ ÿßŸÑÿ≥ŸäÿßŸÇ ÿßŸÑÿ£ŸÅÿ∂ŸÑ\n",
        "    best_context = retrieve_context(query)\n",
        "\n",
        "    # 2. Generate\n",
        "    # TODO: Call your 'generate_answer' function\n",
        "    # ÿßŸÑÿÆÿ∑Ÿàÿ© 2: ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ ÿØÿßŸÑÿ© ÿßŸÑÿ™ŸàŸÑŸäÿØ ŸÑÿßÿ≥ÿ™ÿÆÿ±ÿßÿ¨ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ŸÖŸÜ ÿßŸÑÿ≥ÿ§ÿßŸÑ ŸàÿßŸÑÿ≥ŸäÿßŸÇ\n",
        "    final_answer = generate_answer(question=query, context=best_context)\n",
        "\n",
        "    # 3. Return\n",
        "    # ÿ•ÿ±ÿ¨ÿßÿπ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿßŸÑŸÜŸáÿßÿ¶Ÿäÿ© ŸàÿßŸÑÿ≥ŸäÿßŸÇ ÿßŸÑÿ∞Ÿä ÿ™ŸÖ ÿßÿ≥ÿ™ÿÆÿØÿßŸÖŸá\n",
        "    return final_answer, best_context\n",
        "\n",
        "# --- Verification ---\n",
        "print(\"Testing the full RAG pipeline...\")\n",
        "print(\"Query: 'What is the capital of France?'\")\n",
        "final_answer, retrieved_context = ask_rag_pipeline(\"What is the capital of France?\")\n",
        "\n",
        "print(f\" ¬† -> Retrieved Context: '{retrieved_context}'\")\n",
        "print(f\" ¬† -> Final Answer: '{final_answer}'\")\n",
        "\n",
        "if final_answer.lower() == \"paris\":\n",
        "    print(\"‚úÖ Success! Your RAG pipeline is working!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è RAG pipeline failed. Expected 'Paris'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHtQ-pWlpzga",
        "outputId": "8d185eb6-912e-448b-9357-60df12a29a70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Task 4: Building the Full RAG Pipeline ---\n",
            "Testing the full RAG pipeline...\n",
            "Query: 'What is the capital of France?'\n",
            " ¬† -> Retrieved Context: 'The capital of France is Paris, which is known for the Eiffel Tower.'\n",
            " ¬† -> Final Answer: 'Paris'\n",
            "‚úÖ Success! Your RAG pipeline is working!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### üß™ Self-Assessment\n",
        "\n",
        "Run this final cell to test your complete RAG pipeline. This assessment will:\n",
        "1.  Add new information to the agent's knowledge base.\n",
        "2.  Ask questions that *require* the RAG pipeline to work.\n",
        "3.  It will check if your `retrieve_context` function finds the right document AND if your `generate_answer` function extracts the correct answer.\n",
        "\n",
        "Good luck!\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "KmDt8jCyp32R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # Code Cell 8: Self-Assessment\n",
        "print(\"\\n--- üß™ Self-Assessment ---\")\n",
        "\n",
        "# We will add new documents to the knowledge base and test the agent.\n",
        "# This simulates expanding the agent's memory.\n",
        "\n",
        "try:\n",
        "    # --- Setup for Test ---\n",
        "    new_knowledge = [\n",
        "        \"The currency of Japan is the Yen.\",\n",
        "        \"Maverick is a clever Border Collie who knows many tricks.\",\n",
        "        \"The highest mountain in the world is Mount Everest.\"\n",
        "    ]\n",
        "    # Update all the pieces:\n",
        "    # 1. Update the text list\n",
        "    knowledge_base.extend(new_knowledge)\n",
        "    # 2. Update the embeddings (re-encode everything)\n",
        "    knowledge_embeddings = retriever_model.encode(knowledge_base, convert_to_tensor=True)\n",
        "\n",
        "    print(f\"üìö Agent memory updated. Total documents: {len(knowledge_base)}\")\n",
        "\n",
        "    # --- Test Cases ---\n",
        "    test_cases = [\n",
        "        {\n",
        "            \"query\": \"What is the currency of Japan?\",\n",
        "            \"expected_context_keyword\": \"Japan\",\n",
        "            \"expected_answer_keyword\": \"Yen\"\n",
        "        },\n",
        "        {\n",
        "            \"query\": \"What kind of dog is Maverick?\",\n",
        "            \"expected_context_keyword\": \"Maverick\",\n",
        "            \"expected_answer_keyword\": \"Border Collie\"\n",
        "        },\n",
        "        {\n",
        "            \"query\": \"Who was the first person on the Moon?\",\n",
        "            \"expected_context_keyword\": \"Moon\",\n",
        "            \"expected_answer_keyword\": \"Neil Armstrong\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    score = 0\n",
        "    total = len(test_cases) * 2 # Each test has a retrieval and generation part\n",
        "\n",
        "    for i, test in enumerate(test_cases):\n",
        "        print(f\"\\n--- Test Case {i+1} ---\")\n",
        "        query = test[\"query\"]\n",
        "        print(f\"Query: \\\"{query}\\\"\")\n",
        "\n",
        "        # Run the full pipeline\n",
        "        answer, context = ask_rag_pipeline(query)\n",
        "\n",
        "        # Check Retrieval\n",
        "        print(f\"   -> üîé Retrieved: '{context}'\")\n",
        "        if test[\"expected_context_keyword\"] in context:\n",
        "            print(\"   -> ‚úÖ Retrieval Correct!\")\n",
        "            score += 1\n",
        "        else:\n",
        "            print(f\"   -> ‚ùå Retrieval Failed. Expected context with: '{test['expected_context_keyword']}'\")\n",
        "\n",
        "        # Check Generation\n",
        "        print(f\"   -> ‚úçÔ∏è Answer: '{answer}'\")\n",
        "        if test[\"expected_answer_keyword\"].lower() in answer.lower():\n",
        "            print(\"   -> ‚úÖ Generation Correct!\")\n",
        "            score += 1\n",
        "        else:\n",
        "            print(f\"   -> ‚ùå Generation Failed. Expected answer with: '{test['expected_answer_keyword']}'\")\n",
        "\n",
        "    # Final Score\n",
        "    print(f\"\\n--- üèÅ Assessment Complete ---\")\n",
        "    print(f\"üéØ Your Final Score: {score} / {total}\")\n",
        "    if score == total:\n",
        "        print(\"üéâüéâüéâ Perfect! You have successfully built and tested a full RAG pipeline!\")\n",
        "    elif score >= total // 2:\n",
        "        print(\"üëç Great job! Your pipeline is working. Review any failed tests to see what happened.\")\n",
        "    else:\n",
        "        print(\"üîß Keep trying! Check your functions in Tasks 2, 3, and 4.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n--- ‚ö†Ô∏è Assessment Failed ---\")\n",
        "    print(f\"An error occurred: {e}\")\n",
        "    print(\"Please check your code in all tasks and try again.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "MwKVfPYYp7Pd",
        "outputId": "9aec1543-332d-44a3-f3ba-dfed15f9f323"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- üß™ Self-Assessment ---\n",
            "üìö Agent memory updated. Total documents: 8\n",
            "\n",
            "--- Test Case 1 ---\n",
            "Query: \"What is the currency of Japan?\"\n",
            "   -> üîé Retrieved: 'The currency of Japan is the Yen.'\n",
            "   -> ‚úÖ Retrieval Correct!\n",
            "   -> ‚úçÔ∏è Answer: 'Yen'\n",
            "   -> ‚úÖ Generation Correct!\n",
            "\n",
            "--- Test Case 2 ---\n",
            "Query: \"What kind of dog is Maverick?\"\n",
            "   -> üîé Retrieved: 'Maverick is a clever Border Collie who knows many tricks.'\n",
            "   -> ‚úÖ Retrieval Correct!\n",
            "   -> ‚úçÔ∏è Answer: 'Border Collie'\n",
            "   -> ‚úÖ Generation Correct!\n",
            "\n",
            "--- Test Case 3 ---\n",
            "Query: \"Who was the first person on the Moon?\"\n",
            "   -> üîé Retrieved: 'The first person to walk on the Moon was Neil Armstrong in 1969.'\n",
            "   -> ‚úÖ Retrieval Correct!\n",
            "   -> ‚úçÔ∏è Answer: 'Neil Armstrong'\n",
            "   -> ‚úÖ Generation Correct!\n",
            "\n",
            "--- üèÅ Assessment Complete ---\n",
            "üéØ Your Final Score: 6 / 6\n",
            "üéâüéâüéâ Perfect! You have successfully built and tested a full RAG pipeline!\n"
          ]
        }
      ]
    }
  ]
}